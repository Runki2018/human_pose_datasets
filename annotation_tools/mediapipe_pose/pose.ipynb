{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、生成关键点\n",
    "+ 使用的MediaPipe Pose做姿态估计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import cv2\n",
    "import json\n",
    "import mediapipe as mp \n",
    "import math\n",
    "from pathlib import  Path\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medipipe 中检测的关键点结构\n",
    "pose_structure = dict(\n",
    "    keypoints=[\n",
    "        'nose',\n",
    "        'left_eye_inner', 'left_eye', 'left_eye_outer',\n",
    "        'right_eye_inner', 'right_eye', 'right_eye_outer',\n",
    "        'left_ear', 'right_ear',\n",
    "        'mouth_left', 'mouth_right',\n",
    "        'left_shoulder', 'right_shoulder',  # 11, 12\n",
    "        'left_elbow', 'right_elbow',        # 13, 14\n",
    "        'left_wrist', 'right_wrist',        # 15, 16\n",
    "        'left_pinky', 'right_pinky',\n",
    "        'left_index', 'right_index',\n",
    "        'left_thumb', 'right_thumb',\n",
    "        'left_hip', 'right_hip',          # 23, 24\n",
    "        'left_knee', 'right_knee',        # 25, 26\n",
    "        'left_ankle', 'right_ankle',      # 27, 28\n",
    "        'left_heel', 'right_heel',\n",
    "        'left_foot_index', 'right_foot_index'\n",
    "    ],\n",
    "    request_kpt_indices = [11, 12,   # 挑选出所需的12个关键点序号\n",
    "                            13, 14,\n",
    "                            15, 16,\n",
    "                            23, 24,\n",
    "                            25, 26,\n",
    "                            27, 28],\n",
    "    skeleton = [[0, 1], [6, 7],\n",
    "                [0, 2], [1, 3],\n",
    "                [2, 4], [3, 5],\n",
    "                [0, 6], [1, 7],\n",
    "                [6, 8], [7, 9],\n",
    "                [8, 10], [9, 11]],  # 12个关键点的骨骼连线\n",
    "    line_color = [(255, 255, 255), (255, 255, 255),\n",
    "                  (255, 0, 0),(255, 0, 0),\n",
    "                  (0, 255, 0), (0, 255, 0),\n",
    "                  (0, 0, 255), (0, 0, 255),\n",
    "                  (255, 255, 0), (255, 255, 0),\n",
    "                  (255, 0, 255), (255, 0, 255)]  # 12个关键点的骨骼连线的颜色\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、可视化视频、并保存可视化后的每一帧，用于后续处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class poseDetector():\n",
    "    \"\"\"\n",
    "        目前仅仅适合单人视频检测, 可以获取关键点和边界框, 通过修改也可以获取图像分割掩码信息。\n",
    "    \"\"\"\n",
    "    def __init__(self, mode=False, upBody=False, smooth=True,\n",
    "                 detection_confidence=0.5, track_confidence=0.5, frame_size=(640, 640)):\n",
    "        self.mode = mode\n",
    "        self.upBody = upBody\n",
    "        self.smooth = smooth   # True 减少抖动\n",
    "        self.detection_confidence = detection_confidence\n",
    "        self.track_confidence = track_confidence\n",
    "        self.mpDraw = mp.solutions.drawing_utils\n",
    "        self.mpPose = mp.solutions.pose\n",
    "        # MediaPipe Pose检测器参数，关闭防抖动，因为防抖动可能会降低动作大幅度变化时的检测精度\n",
    "        self.pose = self.mpPose.Pose(static_image_mode=False, # True：图片模式， false 视频流模式\n",
    "                                    model_complexity=2,       # 0， 1， 2， 越大精度越高，延迟越大\n",
    "                                    smooth_landmarks=False,    # 减少关键点抖动，只对视频流有效\n",
    "                                    enable_segmentation=True,  # 生成图像分割掩码, 这里用于获取人体边界框\n",
    "                                    smooth_segmentation=False,   # 减少分割掩码抖动，只对视频流有效\n",
    "                                    min_detection_confidence=0.5,  # 最小检测阈值， 预测结果大于该值保留\n",
    "                                    min_tracking_confidence=0.5)   # 最小跟踪阈值， 检测结果和跟踪预测结果相似度\n",
    "        self.grid = np.arange(frame_size[0]*frame_size[1]).reshape(frame_size)  # 网格序号，用于找出mask的区域，从而计算bbox\n",
    "        self.mask_thr = 0.2   # 分割得分阈值，大于该值为人体像素\n",
    "        self.thickness = 1\n",
    "\n",
    "    def findPose(self, img, draw=True):\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = self.pose.process(img_rgb)\n",
    "        if results.pose_landmarks and draw:\n",
    "            self.mpDraw.draw_landmarks(\n",
    "                img, \n",
    "                results.pose_landmarks,\n",
    "                self.mpPose.POSE_CONNECTIONS)\n",
    "        return img, results\n",
    "\n",
    "    def findPosition(self, img, results, pose_structure, draw=True):\n",
    "        kpts, xyxy, area = [], [], 0\n",
    "        if results.pose_landmarks:\n",
    "            for idx, lm in enumerate(results.pose_landmarks.landmark):\n",
    "                h, w, c = img.shape\n",
    "                cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                vis = round(lm.visibility, 4)\n",
    "                # print(f\"{cx=}\\t{cy=}\\t{vis=}\")\n",
    "                kpts.append([cx, cy, vis])\n",
    "                if draw and idx in pose_structure['request_kpt_indices']:\n",
    "                    color = (255, 0, 0) if vis > 0.8 else (128, 12, 255)\n",
    "                    cv2.circle(img, (cx, cy), self.thickness+2, color, cv2.FILLED)\n",
    "\n",
    "            kpts = np.array(kpts)[pose_structure['request_kpt_indices']]\n",
    "            if draw:\n",
    "                for pair, color in zip(pose_structure['skeleton'], pose_structure['line_color']):\n",
    "                    # line(img, pt1, pt2, color[, thickness[, lineType[, shift]]]) -> img\n",
    "                    pt1 = int(kpts[pair[0], 0]), int(kpts[pair[0], 1])\n",
    "                    pt2 = int(kpts[pair[1], 0]), int(kpts[pair[1], 1])\n",
    "                    cv2.line(img, pt1, pt2, color, thickness=self.thickness)\n",
    "\n",
    "            area = (results.segmentation_mask > self.mask_thr).size  # 人体分割图面积\n",
    "            if area > 0:\n",
    "                # print(results.segmentation_mask.shape)   # h, w\n",
    "                assert results.segmentation_mask.shape == self.grid.shape, f\"{results.segmentation_mask.shape} != {self.grid.shape}\"\n",
    "                mask = self.grid[results.segmentation_mask > self.mask_thr]   # mask_thr 是正样本的阈值, 大于该值为人体区域， 小于则为背景区域\n",
    "                x = mask % self.grid.shape[1]  \n",
    "                y = mask // self.grid.shape[1]\n",
    "                xy = np.concatenate([x[:, None], y[:, None]], axis=-1)\n",
    "                left_top = xy.min(axis=0)\n",
    "                right_bottom = xy.max(axis=0)\n",
    "                xyxy = [*left_top, *right_bottom]\n",
    "                cv2.rectangle(img, left_top, right_bottom, (255, 0, 0), thickness=self.thickness)  # rectangle(img, pt1, pt2, color[, thickness[, lineType[, shift]]]) -> img\n",
    "                \n",
    "        return kpts, xyxy, area\n",
    "\n",
    "\n",
    "    def findAngle(self, img, kpts, p1, p2, p3, draw=True):\n",
    "        x1, y1 = kpts[p1][1:]\n",
    "        x2, y2 = kpts[p2][1:]\n",
    "        x3, y3 = kpts[p3][1:]\n",
    "        \n",
    "        angle = math.degrees(math.atan2(y3 - y2, x3 - x2) - math.atan2(y1 - y2, x1 - x2))\n",
    "        angle += 360 if angle < 0 else 0\n",
    "        assert  0 <= angle <= 360\n",
    "        if draw:\n",
    "            cv2.line(img, (x1, y1), (x2, y2), (255, 255, 255), 3)\n",
    "            cv2.line(img, (x3, y3), (x2, y2), (255, 255, 255), 3)\n",
    "            cv2.circle(img, (x1, y1), 10, (0, 0, 255), cv2.FILLED)\n",
    "            cv2.circle(img, (x1, y1), 15, (0, 0, 255), 2)\n",
    "            cv2.circle(img, (x2, y2), 10, (0, 0, 255), cv2.FILLED)\n",
    "            cv2.circle(img, (x2, y2), 15, (0, 0, 255), 2)\n",
    "            cv2.circle(img, (x2, y2), 10, (0, 0, 255), cv2.FILLED)\n",
    "            cv2.circle(img, (x2, y2), 15, (0, 0, 255), 2)\n",
    "    \n",
    "        return angle        \n",
    "\n",
    "\n",
    "def save_annotations(filename, kpts, bbox, height, width, area):\n",
    "    json_dict = dict(\n",
    "        height=height,\n",
    "        width=width,\n",
    "        area=area,\n",
    "        keypoints=kpts.flatten().tolist(),\n",
    "        bbox=bbox   # xywh\n",
    "    )\n",
    "    with open(str(filename), 'w') as fd:\n",
    "        json.dump(json_dict, fd, indent=4)\n",
    "\n",
    "def main(video_path:str,                            # 视频路径\n",
    "         pose_dict,                                 # 需要保留和可视化的关键点结构\n",
    "         interval=1,                                # 取帧间隔\n",
    "         save=False,                                # 是否保存图片和标注\n",
    "         save_img_root='images',                    # 保存图片的根目录, 最终目录是 save_img_root/<video_name>, 图片保存为 jpg格式        \n",
    "         save_ann_root='annotations',               # 保存标注文件的更目录, 最终目录是 save_ann_root/<video_name>, 标注保存为json格式\n",
    "         show=True):                                # 是否显示可视化\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if show:\n",
    "            cv2.namedWindow(video_path, cv2.WINDOW_KEEPRATIO)\n",
    "        \n",
    "        # 检查和创建输出目录\n",
    "        if save:\n",
    "            img_path = Path(save_img_root).joinpath(Path(video_path).stem)\n",
    "            if not img_path.exists():\n",
    "                img_path.mkdir(mode=777, parents=True, exist_ok=True)\n",
    "            \n",
    "            ann_path = Path(save_ann_root).joinpath(Path(video_path).stem)\n",
    "            if not ann_path.exists():\n",
    "                ann_path.mkdir(mode=777, parents=True, exist_ok=True)\n",
    "\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        frames_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        print(f\"{video_path=}\\n{width=}\\n{height=}\\n{frames_count=}\")\n",
    "        detector = poseDetector(frame_size=(height, width))\n",
    "\n",
    "        for frame_idx in tqdm(range(frames_count), desc=video_path):\n",
    "            if frame_idx % interval == 0 and cap.isOpened():\n",
    "                # if i > 60:\n",
    "                #     break\n",
    "                _, img = cap.read()\n",
    "                img, results = detector.findPose(img, draw=False)\n",
    "                kpts, xyxy, area = detector.findPosition(img, results, pose_structure=pose_dict, draw=True)  # landmark\n",
    "                \n",
    "                # add info on image\n",
    "                cv2.putText(img, str(frame_idx), (50, 50), cv2.FONT_HERSHEY_PLAIN, 2, (255, 0, 0), 3)\n",
    "                if xyxy != []:\n",
    "                    bbox = [int(b) for b in xyxy]\n",
    "                    bbox = [bbox[0], bbox[1], bbox[2] - bbox[0], bbox[3] - bbox[1]]   # x1y1x2y2 -> x1y1wh\n",
    "                    if bbox[2]/bbox[3] > 1.2:   # 宽大于高度， 认为是跌倒\n",
    "                        cv2.putText(img, \"Fall\", (50, 80), cv2.FONT_HERSHEY_PLAIN, 2, (0, 0, 255), 3)\n",
    "                    else:\n",
    "                        cv2.putText(img, \"Normal\", (50, 80), cv2.FONT_HERSHEY_PLAIN, 2, (255, 0, 0), 3)\n",
    "                    \n",
    "                \n",
    "                if save and not isinstance(kpts, list):\n",
    "                    img_file = img_path.joinpath(str(frame_idx) + '.jpg')\n",
    "                    cv2.imwrite(str(img_file), img)\n",
    "                    ann_file = ann_path.joinpath(str(frame_idx) + '.json')\n",
    "                    save_annotations(ann_file, kpts, bbox, height, width, area)\n",
    "\n",
    "                if show:\n",
    "                    cv2.imshow(video_path, img)\n",
    "                    cv2.waitKey(1)\n",
    "\n",
    "    finally:\n",
    "        cv2.destroyAllWindows()\n",
    "        cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1、输入单个视频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(\n",
    "    video_path=\"videos/1.mp4\",    # 视频路径\n",
    "    pose_dict=pose_structure,     # 需要保留和可视化的关键点结构\n",
    "    interval=1,                   # 取帧间隔\n",
    "    save=True,                    # 是否保存图片和标注\n",
    "    save_img_root='images',       # 保存图片的根目录, 最终目录是 save_img_root/<video_name>, 图片保存为 jpg格式\n",
    "    save_ann_root='annotations',  # 保存标注文件的更目录, 最终目录是 save_ann_root/<video_name>, 标注保存为json格式\n",
    "    show=False                    # 是否显示可视化\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 多个视频的根目录\n",
    "# video_path = Path(\"FallDataset\")\n",
    "# # 视频的格式如： '.avi', '.mp4'\n",
    "# video_format = '.avi'\n",
    "# i = 1 \n",
    "# for video in video_path.glob('**/*' + video_format):\n",
    "#     print(f\"{i=}\\t{video.parts[-1]=}\")\n",
    "#     i+= 1\n",
    "from datetime import datetime as dt\n",
    "dt.strftime(dt.now(), \"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2、输入多个视频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多个视频的根目录\n",
    "video_path = Path(\"FallDataset\")\n",
    "# 视频的格式如： '.avi', '.mp4'\n",
    "video_format = '.avi'\n",
    "for video in video_path.glob('**/*' + video_format):\n",
    "    main(\n",
    "        video_path=str(video),        # 视频路径\n",
    "        pose_dict=pose_structure,     # 需要保留和可视化的关键点结构\n",
    "        interval=3,                   # 取帧间隔\n",
    "        save=True,                   # 是否保存图片和标注\n",
    "        save_img_root='images',       # 保存图片的根目录, 最终目录是 save_img_root/<video_name>, 图片保存为 jpg格式\n",
    "        save_ann_root='annotations',  # 保存标注文件的更目录, 最终目录是 save_ann_root/<video_name>, 标注保存为json格式\n",
    "        show=False                     # 是否显示可视化\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_annotations(img, bbox, kpts):\n",
    "    for i in range(0, len(kpts), 3):\n",
    "        cv2.circle(img, (int(kpts[i]), int(kpts[i+1])), 6, (0, 255, 0), cv2.FILLED)\n",
    "    left_top = bbox[:2]\n",
    "    right_bottom = bbox[0] + bbox[2], bbox[1] + bbox[3]\n",
    "    # rectangle(img, pt1, pt2, color[, thickness[, lineType[, shift]]]) -> img\n",
    "    cv2.rectangle(img, left_top, right_bottom, (0, 255, 0), thickness=2)  \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、生成最终数据集\n",
    "1. 使用Windows图片查看器去除检测结果质量低的图片，快捷键: '→'键 下一张, 'Del'键 删除当前图片\n",
    "2. 使用merge_remained_data函数，根据保留下来的高质量检测结果，生成相应帧图片和标注文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_remained_data(video_root='videos',              # 存放视频的根目录\n",
    "                        img_root='images',                # 存放高质量可视化图片的根目录，先使用window图片查看器去除低质量检测图片\n",
    "                        ann_root='annotations',           # 存放标注文件的更目录\n",
    "                        output_root='merge_datasets',     # 输出图片和标注文件的根目录。根据img_root中保留的图片，生成相应帧图片和标注文件\n",
    "                        show=False,                       # 可视化帧与标注，查看是否正确匹配\n",
    "                        ):\n",
    "\n",
    "    # 输入目录\n",
    "    video_root = Path(video_root)\n",
    "    img_dirs = os.listdir(img_root)   # 图片目录中我们手工删除检测质量较差的结果\n",
    "    ann_dirs = os.listdir(ann_root)\n",
    "    img_dirs = [d for d in img_dirs if os.path.isdir(os.path.join(img_root, d))]\n",
    "    ann_dirs = [d for d in ann_dirs if os.path.isdir(os.path.join(img_root, d))]\n",
    "    print(f\"{img_dirs=}\\t{ann_dirs=}\")\n",
    "    dirs = list(set(img_dirs) and set(ann_dirs))  # 共同视频目录\n",
    "    video_names = {name.split('.')[0]:name for name in os.listdir(video_root) if name.split('.')[0] in dirs}\n",
    "    print(f\"common directories: {dirs}\")\n",
    "\n",
    "    # 输出目录\n",
    "    output_root = Path(output_root)\n",
    "    out_img_path = output_root.joinpath('images')\n",
    "    out_ann_path = output_root.joinpath('annotations')\n",
    "    if not out_img_path.exists():\n",
    "        out_img_path.mkdir(mode=777, parents=True, exist_ok=True)\n",
    "    if not out_ann_path.exists():\n",
    "        out_ann_path.mkdir(mode=777, parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"Save images\\t\\t=> {out_img_path}\")\n",
    "    print(f\"Save annotations\\t=> {out_ann_path}\")\n",
    "\n",
    "    # 开始比对，如果合并保留下来的高质量检测结果。\n",
    "    img_id, ann_id = 0, 0\n",
    "    images, annotations = [], []   # Json标注文件中的\n",
    "    # frame_ids= []  # 保留下来的视频帧的id\n",
    "    for _dir in dirs:\n",
    "        try:\n",
    "            img_files = os.listdir(os.path.join(img_root, _dir))\n",
    "            ann_files = os.listdir(os.path.join(ann_root, _dir))\n",
    "            hash_table = {a.strip('.json'):a for a in ann_files if '.json' in a}\n",
    "            cap = cv2.VideoCapture(str(video_root.joinpath(video_names[_dir])))\n",
    "\n",
    "            # 按帧序号升序排序\n",
    "            img_files = sorted(img_files, key=lambda x:int(x.strip('.jpg')))\n",
    "            \n",
    "            # 匹配和生成标注\n",
    "            for file in tqdm(img_files, desc=f\"Directory => {_dir}\"):\n",
    "                frame_id = file.strip('.jpg')\n",
    "                ann_file = hash_table.get(frame_id, None)\n",
    "                if ann_file != None:\n",
    "                    with open(os.path.join(ann_root, _dir, ann_file), 'r') as fd:\n",
    "                        ann_dict = json.load(fd)\n",
    "                    # frame_ids.append(frame_dis)\n",
    "                    cap.set(cv2.CAP_PROP_POS_FRAMES, int(frame_id))\n",
    "                    success, img = cap.read()\n",
    "                    if not success:\n",
    "                        continue\n",
    "                    \n",
    "                    # copy image to output path\n",
    "                    dst_file = str(img_id) + '.jpg'\n",
    "                    dst_img = out_img_path / dst_file\n",
    "                    cv2.imwrite(str(dst_img), img)\n",
    "                    \n",
    "                    # src_img = Path(img_root) / _dir / file\n",
    "                    # shutil.copyfile(src=src_img, dst=dst_img)\n",
    "                    images.append(dict(\n",
    "                        id=img_id,\n",
    "                        file_name=dst_file,\n",
    "                        width=ann_dict['width'],\n",
    "                        height=ann_dict['height'],\n",
    "                    ))\n",
    "                    annotations.append(dict(\n",
    "                        id=ann_id,\n",
    "                        image_id=img_id,\n",
    "                        category_id=1,\n",
    "                        bbox=ann_dict['bbox'],\n",
    "                        area=int(ann_dict['area']),\n",
    "                        num_keypoints=int(sum(np.array(ann_dict['keypoints'][2::3]) > 0.5)),\n",
    "                        keypoints=ann_dict['keypoints'],\n",
    "                        iscrowd=0,\n",
    "                    ))\n",
    "                    img_id += 1\n",
    "                    ann_id += 1\n",
    "                    \n",
    "                    if show:\n",
    "                        vis_annotations(img, ann_dict['bbox'], ann_dict['keypoints'])\n",
    "                        cv2.imshow('image', img)\n",
    "                        cv2.waitKey(1)\n",
    "        finally:\n",
    "            cv2.destroyAllWindows()\n",
    "            cap.release()\n",
    "\n",
    "    # 保存COCO数据集标注格式的JSON标注文件\n",
    "    json_file = out_ann_path.joinpath(\"video_keypoints.json\")\n",
    "    print(f\"Saving annotation file\\t=> {json_file}\")\n",
    "    with json_file.open('w') as fd:\n",
    "        json_dict = dict(\n",
    "            categories=[{\n",
    "                \"supercategory\": \"person\",\n",
    "                \"id\": 1,\n",
    "                \"name\": \"person\",\n",
    "                \"keypoints\": [\n",
    "                    \"left_shoulder\", \"right_shoulder\",\n",
    "                    \"left_elbow\",\"right_elbow\",\n",
    "                    \"left_wrist\",\"right_wrist\",\n",
    "                    \"left_hip\",\"right_hip\",\n",
    "                    \"left_knee\",\"right_knee\",\n",
    "                    \"left_ankle\",\"right_ankle\"\n",
    "                ],\n",
    "                \"skeleton\": [[0, 1], [6, 7],\n",
    "                            [0, 2], [1, 3],\n",
    "                            [2, 4], [3, 5],\n",
    "                            [0, 6], [1, 7],\n",
    "                            [6, 8], [7, 9],\n",
    "                            [8, 10], [9, 11]]\n",
    "            }],\n",
    "            images=images,\n",
    "            annotations=annotations)\n",
    "        json.dump(json_dict, fd, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 运行脚本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_remained_data(\n",
    "    video_root=\"videos\",\n",
    "    img_root='images',\n",
    "    ann_root='annotations',\n",
    "    output_root='merge_datasets',\n",
    "    show=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "a = Path('vis_images/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.is_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Path(\"a/b/c.jpg\")\n",
    "a.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.83785303, 0.98286794],\n",
       "        [0.97759386, 0.44165153]]),\n",
       " array([[0.26357303, 0.25866853],\n",
       "        [0.60021287, 0.25537808]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.rand(2, 3)[:, :2]\n",
    "b = np.random.rand(2, 3)[:, :2]\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92426311, 0.42084939])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm((a-b), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 2]]), array([[0, 2]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 2]])\n",
    "b = np.array([[0, 2]])\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.linalg.norm((a-b), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[a[:, 1]>3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('p39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2977251e1b2ca22a3cfc09d5f408daba24a1809e852c29e73dfa9b1ba1933376"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
